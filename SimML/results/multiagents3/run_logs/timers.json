{
    "name": "root",
    "gauges": {
        "HunterController.Policy.Entropy.mean": {
            "value": 1.4098258018493652,
            "min": 1.4098258018493652,
            "max": 1.4295159578323364,
            "count": 22
        },
        "HunterController.Policy.Entropy.sum": {
            "value": 14008.029296875,
            "min": 13887.0859375,
            "max": 15256.4248046875,
            "count": 22
        },
        "HunterController.Environment.EpisodeLength.mean": {
            "value": 58.53048780487805,
            "min": 56.58757062146893,
            "max": 317.09375,
            "count": 22
        },
        "HunterController.Environment.EpisodeLength.sum": {
            "value": 9599.0,
            "min": 5995.0,
            "max": 11612.0,
            "count": 22
        },
        "HunterController.Step.mean": {
            "value": 219951.0,
            "min": 9968.0,
            "max": 219951.0,
            "count": 22
        },
        "HunterController.Step.sum": {
            "value": 219951.0,
            "min": 9968.0,
            "max": 219951.0,
            "count": 22
        },
        "HunterController.Policy.ExtrinsicValueEstimate.mean": {
            "value": 1.8146324157714844,
            "min": -0.4145442545413971,
            "max": 1.8146324157714844,
            "count": 22
        },
        "HunterController.Policy.ExtrinsicValueEstimate.sum": {
            "value": 430.06787109375,
            "min": -75.22010803222656,
            "max": 434.5387268066406,
            "count": 22
        },
        "HunterController.Environment.CumulativeReward.mean": {
            "value": 8.43558282208589,
            "min": -13.75,
            "max": 8.43558282208589,
            "count": 22
        },
        "HunterController.Environment.CumulativeReward.sum": {
            "value": 1375.0,
            "min": -605.0,
            "max": 1375.0,
            "count": 22
        },
        "HunterController.Policy.ExtrinsicReward.mean": {
            "value": 8.43558282208589,
            "min": -13.75,
            "max": 8.43558282208589,
            "count": 22
        },
        "HunterController.Policy.ExtrinsicReward.sum": {
            "value": 1375.0,
            "min": -605.0,
            "max": 1375.0,
            "count": 22
        },
        "HunterController.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 22
        },
        "HunterController.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 22
        },
        "AgentController.Policy.Entropy.mean": {
            "value": 1.3988544940948486,
            "min": 1.3971507549285889,
            "max": 1.4189692735671997,
            "count": 22
        },
        "AgentController.Policy.Entropy.sum": {
            "value": 13899.0185546875,
            "min": 13723.7373046875,
            "max": 15256.4248046875,
            "count": 22
        },
        "AgentController.Environment.EpisodeLength.mean": {
            "value": 58.53048780487805,
            "min": 56.58757062146893,
            "max": 317.09375,
            "count": 22
        },
        "AgentController.Environment.EpisodeLength.sum": {
            "value": 9599.0,
            "min": 5995.0,
            "max": 11612.0,
            "count": 22
        },
        "AgentController.Step.mean": {
            "value": 219951.0,
            "min": 9968.0,
            "max": 219951.0,
            "count": 22
        },
        "AgentController.Step.sum": {
            "value": 219951.0,
            "min": 9968.0,
            "max": 219951.0,
            "count": 22
        },
        "AgentController.Policy.ExtrinsicValueEstimate.mean": {
            "value": -0.016812240704894066,
            "min": -0.1441531777381897,
            "max": 0.5017128586769104,
            "count": 22
        },
        "AgentController.Policy.ExtrinsicValueEstimate.sum": {
            "value": -3.9845008850097656,
            "min": -26.524185180664062,
            "max": 118.74313354492188,
            "count": 22
        },
        "AgentController.Environment.CumulativeReward.mean": {
            "value": 0.5276073619631901,
            "min": -6.771428571428571,
            "max": 6.803921568627451,
            "count": 22
        },
        "AgentController.Environment.CumulativeReward.sum": {
            "value": 86.0,
            "min": -334.0,
            "max": 518.0,
            "count": 22
        },
        "AgentController.Policy.ExtrinsicReward.mean": {
            "value": 0.5276073619631901,
            "min": -6.771428571428571,
            "max": 6.803921568627451,
            "count": 22
        },
        "AgentController.Policy.ExtrinsicReward.sum": {
            "value": 86.0,
            "min": -334.0,
            "max": 518.0,
            "count": 22
        },
        "AgentController.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 22
        },
        "AgentController.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 22
        },
        "HunterController.Losses.PolicyLoss.mean": {
            "value": 0.020387542458289925,
            "min": 0.01794779582123738,
            "max": 0.02814438126515597,
            "count": 21
        },
        "HunterController.Losses.PolicyLoss.sum": {
            "value": 0.020387542458289925,
            "min": 0.01794779582123738,
            "max": 0.02814438126515597,
            "count": 21
        },
        "HunterController.Losses.ValueLoss.mean": {
            "value": 1.5581624468167623,
            "min": 1.031200212240219,
            "max": 3.361647280057271,
            "count": 21
        },
        "HunterController.Losses.ValueLoss.sum": {
            "value": 1.5581624468167623,
            "min": 1.031200212240219,
            "max": 3.361647280057271,
            "count": 21
        },
        "HunterController.Policy.LearningRate.mean": {
            "value": 0.00026762281079239993,
            "min": 0.00026762281079239993,
            "max": 0.0002984550005150001,
            "count": 21
        },
        "HunterController.Policy.LearningRate.sum": {
            "value": 0.00026762281079239993,
            "min": 0.00026762281079239993,
            "max": 0.0002984550005150001,
            "count": 21
        },
        "HunterController.Policy.Epsilon.mean": {
            "value": 0.18920759999999992,
            "min": 0.18920759999999992,
            "max": 0.19948500000000005,
            "count": 21
        },
        "HunterController.Policy.Epsilon.sum": {
            "value": 0.18920759999999992,
            "min": 0.18920759999999992,
            "max": 0.19948500000000005,
            "count": 21
        },
        "HunterController.Policy.Beta.mean": {
            "value": 0.004461459239999998,
            "min": 0.004461459239999998,
            "max": 0.0049743015,
            "count": 21
        },
        "HunterController.Policy.Beta.sum": {
            "value": 0.004461459239999998,
            "min": 0.004461459239999998,
            "max": 0.0049743015,
            "count": 21
        },
        "AgentController.Losses.PolicyLoss.mean": {
            "value": 0.02524160560181675,
            "min": 0.01668810495757498,
            "max": 0.029887182006496003,
            "count": 21
        },
        "AgentController.Losses.PolicyLoss.sum": {
            "value": 0.02524160560181675,
            "min": 0.01668810495757498,
            "max": 0.029887182006496003,
            "count": 21
        },
        "AgentController.Losses.ValueLoss.mean": {
            "value": 9.135081926981607,
            "min": 1.2748328427473703,
            "max": 9.135081926981607,
            "count": 21
        },
        "AgentController.Losses.ValueLoss.sum": {
            "value": 9.135081926981607,
            "min": 1.2748328427473703,
            "max": 9.135081926981607,
            "count": 21
        },
        "AgentController.Policy.LearningRate.mean": {
            "value": 0.00026762281079239993,
            "min": 0.00026762281079239993,
            "max": 0.0002984550005150001,
            "count": 21
        },
        "AgentController.Policy.LearningRate.sum": {
            "value": 0.00026762281079239993,
            "min": 0.00026762281079239993,
            "max": 0.0002984550005150001,
            "count": 21
        },
        "AgentController.Policy.Epsilon.mean": {
            "value": 0.18920759999999992,
            "min": 0.18920759999999992,
            "max": 0.19948500000000005,
            "count": 21
        },
        "AgentController.Policy.Epsilon.sum": {
            "value": 0.18920759999999992,
            "min": 0.18920759999999992,
            "max": 0.19948500000000005,
            "count": 21
        },
        "AgentController.Policy.Beta.mean": {
            "value": 0.004461459239999998,
            "min": 0.004461459239999998,
            "max": 0.0049743015,
            "count": 21
        },
        "AgentController.Policy.Beta.sum": {
            "value": 0.004461459239999998,
            "min": 0.004461459239999998,
            "max": 0.0049743015,
            "count": 21
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1721368729",
        "python_version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]",
        "command_line_arguments": "D:\\Users\\areya\\Desktop\\MLsimulation\\SimML\\venv\\Scripts\\mlagents-learn config\\multitraining.yaml --run-id multiagents3 --force",
        "mlagents_version": "0.30.0",
        "mlagents_envs_version": "0.30.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "2.3.1+cpu",
        "numpy_version": "1.21.2",
        "end_time_seconds": "1721369042"
    },
    "total": 313.17663729999913,
    "count": 1,
    "self": 0.006694199997582473,
    "children": {
        "run_training.setup": {
            "total": 0.08457150000322144,
            "count": 1,
            "self": 0.08457150000322144
        },
        "TrainerController.start_learning": {
            "total": 313.0853715999983,
            "count": 1,
            "self": 0.21523769955092575,
            "children": {
                "TrainerController._reset_env": {
                    "total": 7.703670700007933,
                    "count": 1,
                    "self": 7.703670700007933
                },
                "TrainerController.advance": {
                    "total": 305.0331966004451,
                    "count": 11233,
                    "self": 0.26027889882971067,
                    "children": {
                        "env_step": {
                            "total": 198.54063060005137,
                            "count": 11233,
                            "self": 177.7885412004107,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 20.622081999856164,
                                    "count": 11233,
                                    "self": 0.9673493997252081,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 19.654732600130956,
                                            "count": 19184,
                                            "self": 19.654732600130956
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 0.13000739978451747,
                                    "count": 11232,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 306.79093859979184,
                                            "count": 11232,
                                            "is_parallel": true,
                                            "self": 151.85265279789746,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.0012032000086037442,
                                                    "count": 2,
                                                    "is_parallel": true,
                                                    "self": 0.0003397000109544024,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.0008634999976493418,
                                                            "count": 8,
                                                            "is_parallel": true,
                                                            "self": 0.0008634999976493418
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 154.93708260188578,
                                                    "count": 11232,
                                                    "is_parallel": true,
                                                    "self": 3.926186000506277,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 4.8434238017653115,
                                                            "count": 11232,
                                                            "is_parallel": true,
                                                            "self": 4.8434238017653115
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 135.2338293001958,
                                                            "count": 11232,
                                                            "is_parallel": true,
                                                            "self": 135.2338293001958
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 10.933643499418395,
                                                            "count": 22464,
                                                            "is_parallel": true,
                                                            "self": 3.059901096072281,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 7.873742403346114,
                                                                    "count": 89856,
                                                                    "is_parallel": true,
                                                                    "self": 7.873742403346114
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 106.232287101564,
                            "count": 22464,
                            "self": 0.6949116021860391,
                            "children": {
                                "process_trajectory": {
                                    "total": 25.949787299337913,
                                    "count": 22464,
                                    "self": 25.949787299337913
                                },
                                "_update_policy": {
                                    "total": 79.58758820004005,
                                    "count": 44,
                                    "self": 54.13063520047581,
                                    "children": {
                                        "TorchPPOOptimizer.update": {
                                            "total": 25.456952999564237,
                                            "count": 1320,
                                            "self": 25.456952999564237
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 7.999915396794677e-07,
                    "count": 1,
                    "self": 7.999915396794677e-07
                },
                "TrainerController._save_models": {
                    "total": 0.13326580000284594,
                    "count": 1,
                    "self": 0.01745020001544617,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.11581559998739976,
                            "count": 2,
                            "self": 0.11581559998739976
                        }
                    }
                }
            }
        }
    }
}